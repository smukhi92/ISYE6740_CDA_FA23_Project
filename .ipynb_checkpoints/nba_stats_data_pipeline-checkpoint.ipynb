{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ddbcc-8e6b-4888-b8ff-f6d260b09daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"local_testing\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    "    )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6fc11e-e131-4e24-9e2f-7ffbcb6aeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Read in Advanced Stats CSV data and filter to only NBA data (removing historical ABA data)\n",
    "adv_stats_file = 'NBA_data/Advanced.csv'\n",
    "# filter down to only NBA data\n",
    "df_adv_stats = spark.read.csv(adv_stats_file, header=True, nullValue=\"NA\", inferSchema=True).where(\"lg = 'NBA'\")\n",
    "# df_adv_stats.createOrReplaceTempView('tmp_advanced_stats')\n",
    "df_adv_stats.printSchema()\n",
    "print(df_adv_stats.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c565d54-f041-4b59-86a6-b0e5230f9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: dedupe data to 1 record per player, per season.\n",
    "# Players who played for multiple teams in a season have 1 record per team and 1 overall summary record (where \"tm = 'TOT\")\n",
    "# We only care about the overall summary record\n",
    "\n",
    "# overall summary record for players with multiple teams in the season\n",
    "df_multi_team = df_adv_stats.where(\"tm = 'TOT'\")\n",
    "\n",
    "# summary record for players only on one team in a season\n",
    "df_single_team = df_adv_stats.join(df_multi_team, on=[\"player_id\", \"season\"], how=\"left_anti\")\n",
    "\n",
    "# union them together\n",
    "df_all = df_multi_team.unionByName(df_single_team)\n",
    "df_all.createOrReplaceTempView(\"tmp_advanced_stats_deduped\")\n",
    "print(df_all.count())\n",
    "\n",
    "# Confirm each player only has one record per season played\n",
    "dupe_count = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        player_id,\n",
    "        player,\n",
    "        season,\n",
    "        COUNT(*) AS record_count\n",
    "    FROM tmp_advanced_stats_deduped\n",
    "    GROUP BY ALL\n",
    "    HAVING COUNT(*) > 1\n",
    "    \"\"\").count()\n",
    "\n",
    "assert dupe_count == 0, \"Duplicate records found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a368767-f04e-499d-84d5-2705d4ba29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Filter down to only players who meet the following criteria:\n",
    "## earliest season was 1975 onward\n",
    "## and who played a minimum of 2 seasons (41 games each)\n",
    "## filter the seasons down to 1980 onward as it is concerned the \"modern era\"\n",
    "df_season_filtered = spark.sql(\"\"\"\n",
    "    WITH cte_season_count_filter AS (\n",
    "        SELECT\n",
    "            player_id,\n",
    "            player,\n",
    "            COUNT(DISTINCT season) AS total_seasons\n",
    "        FROM\n",
    "            tmp_advanced_stats_deduped\n",
    "        WHERE\n",
    "            g >= 41\n",
    "        GROUP BY ALL\n",
    "        HAVING COUNT(DISTINCT season) >= 2\n",
    "        AND MIN(season) >= 1975\n",
    "        ORDER BY 3 DESC\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        tas.*,\n",
    "        tas.mp/tas.g AS mpg,\n",
    "        ows/ws * ws_48 AS ows_48,\n",
    "        dws/ws * ws_48 AS dws_48\n",
    "    FROM\n",
    "        tmp_advanced_stats_deduped tas\n",
    "    LEFT SEMI JOIN\n",
    "        cte_season_count_filter cscf\n",
    "        USING (player_id)\n",
    "    WHERE\n",
    "        season >= 1980\n",
    "        AND g >= 41\n",
    "    \"\"\")\n",
    "\n",
    "df_season_filtered.createOrReplaceTempView('tmp_season_filtered')\n",
    "print(df_season_filtered.count())\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202d628-15af-4b7c-b528-7be962cfd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Get the per-season league-wide mean and standard deviations for each advanced stat of interest.\n",
    "# This will be used to standardize the performance of each player with regards to their peers *for a single season*\n",
    "df_season_avg = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        season,\n",
    "        ROUND(AVG(ts_percent), 3) AS avg_ts_percent,\n",
    "        ROUND(AVG(per), 3) AS avg_per,\n",
    "        ROUND(AVG(mpg), 3) AS avg_mpg,\n",
    "        ROUND(AVG(x3p_ar), 3) AS avg_x3p_ar,\n",
    "        ROUND(AVG(orb_percent), 3) AS avg_orb_percent,\n",
    "        ROUND(AVG(drb_percent), 3) AS avg_drb_percent,\n",
    "        ROUND(AVG(ast_percent), 3) AS avg_ast_percent,\n",
    "        ROUND(AVG(stl_percent), 3) AS avg_stl_percent,\n",
    "        ROUND(AVG(blk_percent), 3) AS avg_blk_percent,\n",
    "        ROUND(AVG(tov_percent), 3) AS avg_tov_percent,\n",
    "        ROUND(AVG(usg_percent), 3) AS avg_usg_percent,\n",
    "        ROUND(AVG(ows_48), 3) AS avg_ows_48,\n",
    "        ROUND(AVG(dws_48), 3) AS avg_dws_48,\n",
    "        ROUND(AVG(obpm), 3) AS avg_obpm,\n",
    "        ROUND(AVG(dbpm), 3) AS avg_dbpm,\n",
    "        ROUND(AVG(bpm), 3) AS avg_bpm,\n",
    "        ROUND(AVG(vorp), 3) AS avg_vorp,\n",
    "\n",
    "        ROUND(STD(ts_percent), 3) AS std_ts_percent,\n",
    "        ROUND(STD(per), 3) AS std_per,\n",
    "        ROUND(STD(mp/g), 3) AS std_mpg,\n",
    "        ROUND(STD(x3p_ar), 3) AS std_x3p_ar,\n",
    "        ROUND(STD(orb_percent), 3) AS std_orb_percent,\n",
    "        ROUND(STD(drb_percent), 3) AS std_drb_percent,\n",
    "        ROUND(STD(ast_percent), 3) AS std_ast_percent,\n",
    "        ROUND(STD(stl_percent), 3) AS std_stl_percent,\n",
    "        ROUND(STD(blk_percent), 3) AS std_blk_percent,\n",
    "        ROUND(STD(tov_percent), 3) AS std_tov_percent,\n",
    "        ROUND(STD(usg_percent), 3) AS std_usg_percent,\n",
    "        ROUND(STD(ows_48), 3) AS std_ows_48,\n",
    "        ROUND(STD(dws_48), 3) AS std_dws_48,\n",
    "        ROUND(STD(obpm), 3) AS std_obpm,\n",
    "        ROUND(STD(dbpm), 3) AS std_dbpm,\n",
    "        ROUND(STD(bpm), 3) AS std_bpm,\n",
    "        ROUND(STD(vorp), 3) AS std_vorp\n",
    "    FROM\n",
    "        tmp_season_filtered\n",
    "    WHERE tm != 'TOT'\n",
    "    GROUP BY season\n",
    "    ORDER BY season\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "df_season_avg.createOrReplaceTempView(\"tmp_season_avg\")\n",
    "display(df_season_avg.toPandas()) #show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54586148-81c9-472f-bf49-21bde1ae7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize each feature per-player-per-season using the overall mean and standard deviation of each feature per season\n",
    "df_standardized = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        player,\n",
    "        player_id,\n",
    "        season,\n",
    "        g,\n",
    "        ROUND((ts_percent - avg_ts_percent)/std_ts_percent, 3) AS ts_percent,\n",
    "        ROUND((per - avg_per)/std_per, 3) AS per,\n",
    "        ROUND((mpg - avg_mpg)/std_mpg, 3) AS mpg,\n",
    "        ROUND((x3p_ar - avg_x3p_ar)/std_x3p_ar, 3) AS x3p_ar,\n",
    "        ROUND((orb_percent - avg_orb_percent)/std_orb_percent, 3) AS orb_percent,\n",
    "        ROUND((drb_percent - avg_drb_percent)/std_drb_percent, 3) AS drb_percent,\n",
    "        ROUND((ast_percent - avg_ast_percent)/std_ast_percent, 3) AS ast_percent,\n",
    "        ROUND((stl_percent - avg_stl_percent)/std_stl_percent, 3) AS stl_percent,\n",
    "        ROUND((blk_percent - avg_blk_percent)/std_blk_percent, 3) AS blk_percent,\n",
    "        ROUND((tov_percent - avg_tov_percent)/std_tov_percent, 3) AS tov_percent,\n",
    "        ROUND((usg_percent - avg_usg_percent)/std_usg_percent, 3) AS usg_percent,\n",
    "        ROUND((ows_48 - avg_ows_48)/std_ows_48, 3) AS ows_48,\n",
    "        ROUND((dws_48 - avg_dws_48)/std_dws_48, 3) AS dws_48,\n",
    "        ROUND((obpm - avg_obpm)/std_obpm, 3) AS obpm,\n",
    "        ROUND((dbpm - avg_dbpm)/std_dbpm, 3) AS dbpm,\n",
    "        ROUND((bpm - avg_bpm)/std_bpm, 3) AS bpm,\n",
    "        ROUND((vorp - avg_vorp)/std_vorp, 3) AS vorp\n",
    "    FROM\n",
    "        tmp_season_filtered tsf\n",
    "    INNER JOIN\n",
    "        tmp_season_avg tsa\n",
    "        USING (season)\n",
    "    \"\"\")\n",
    "df_standardized.createOrReplaceTempView(\"tmp_standardized\")\n",
    "\n",
    "display(df_standardized.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10bc35-d2fe-4180-b167-6f9bb5100e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    player_id,\n",
    "    player,\n",
    "    SUM(g) AS games_played,\n",
    "    COUNT(DISTINCT season) AS seasons_played,\n",
    "    ROUND(AVG(ts_percent), 3) AS avg_ts_percent,\n",
    "    ROUND(AVG(per), 3) AS avg_per,\n",
    "    ROUND(AVG(mpg), 3) AS avg_mpg,\n",
    "    ROUND(AVG(x3p_ar), 3) AS avg_x3p_ar,\n",
    "    ROUND(AVG(orb_percent), 3) AS avg_orb_percent,\n",
    "    ROUND(AVG(drb_percent), 3) AS avg_drb_percent,\n",
    "    ROUND(AVG(ast_percent), 3) AS avg_ast_percent,\n",
    "    ROUND(AVG(stl_percent), 3) AS avg_stl_percent,\n",
    "    ROUND(AVG(blk_percent), 3) AS avg_blk_percent,\n",
    "    ROUND(AVG(tov_percent), 3) AS avg_tov_percent,\n",
    "    ROUND(AVG(usg_percent), 3) AS avg_usg_percent,\n",
    "    ROUND(AVG(ows_48), 3) AS avg_ows_48,\n",
    "    ROUND(AVG(dws_48), 3) AS avg_dws_48,\n",
    "    ROUND(AVG(obpm), 3) AS avg_obpm,\n",
    "    ROUND(AVG(dbpm), 3) AS avg_dbpm,\n",
    "    ROUND(AVG(bpm), 3) AS avg_bpm,\n",
    "    ROUND(AVG(vorp), 3) AS avg_vorp,\n",
    "    ROUND(MAX(ts_percent), 3) AS max_ts_percent,\n",
    "    ROUND(MAX(per), 3) AS max_per,\n",
    "    ROUND(MAX(mpg), 3) AS max_mpg,\n",
    "    ROUND(MAX(x3p_ar), 3) AS max_x3p_ar,\n",
    "    ROUND(MAX(orb_percent), 3) AS max_orb_percent,\n",
    "    ROUND(MAX(drb_percent), 3) AS max_drb_percent,\n",
    "    ROUND(MAX(ast_percent), 3) AS max_ast_percent,\n",
    "    ROUND(MAX(stl_percent), 3) AS max_stl_percent,\n",
    "    ROUND(MAX(blk_percent), 3) AS max_blk_percent,\n",
    "    ROUND(MAX(tov_percent), 3) AS max_tov_percent,\n",
    "    ROUND(MAX(usg_percent), 3) AS max_usg_percent,\n",
    "    ROUND(MAX(ows_48), 3) AS max_ows_48,\n",
    "    ROUND(MAX(dws_48), 3) AS max_dws_48,\n",
    "    ROUND(MAX(obpm), 3) AS max_obpm,\n",
    "    ROUND(MAX(dbpm), 3) AS max_dbpm,\n",
    "    ROUND(MAX(bpm), 3) AS max_bpm,\n",
    "    ROUND(MAX(vorp), 3) AS max_vorp\n",
    "FROM\n",
    "    tmp_standardized\n",
    "GROUP BY ALL\n",
    "\"\"\")\n",
    "\n",
    "print(df_final.count())\n",
    "pdf_final = df_final.toPandas()\n",
    "display(pdf_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63b763-109b-45a3-81a9-9e77a7a587f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_final.to_csv('cleaned_data/nba_cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a580ad-28df-4625-ac15-ea41e01f8eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
